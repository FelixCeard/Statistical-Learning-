\documentclass{article}
\usepackage{import}


\title{Statisitcal Learning and Stochastic Processes \\Sheet 01 }
\author{Felix CÃ©ard-Falkenberg\\7174020}

\usepackage{homework} % See homework.sty %
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[utf8]{inputenc}
\newcommand*\dif{\mathop{}\mathrm{d}}
\newcommand{\stepandtag}{%
  \refstepcounter{equation}%
  \tag{\theequation}%
}
\newcommand{\var}{\ensuremath{\text{Var}}}


\begin{document}


\section*{Problem 1}
\subsection*{(a)}
\textit{Find the $\mathbb{E}[ \bar{X}(N)]$ analytically. Does it really estimate the mean
of $X$?}

We have that \begin{equation*}
    \bar{X}(N) = \frac{1}{N} \sum_{i=1}^N X_i, \quad X_i \sim \mathrm{Uniform}(0, 1)\ .
\end{equation*}

We first compute the expectation of $X_i$. We have
\begin{align*}
    \mathbb{E}\left[ X_i \right] &= \int_{-\infty}^{\infty} x \cdot p(X=x)\; \dif x \\
    &= \int_{0}^{1} x \cdot p(X=x) \dif x \\
    &= \int_{0}^{1} x \cdot \frac{1}{1-0} \dif x \\
    &= \int_{0}^{1} x \dif x \\
    &= \left[ \frac{1}{2} x^{2} \right]_0^1 \\
    &= \frac{1}{2} (1^2 - 0^2) \\
    &= \frac{1}{2} \stepandtag \label{eq:expected_X_i}
\end{align*}


Therefore, we can compute the expectation of $\bar{X}(N)$ as follows:
\begin{align*}
    \mathbb{E}[\bar{X}(N)] &= \mathbb{E}\left[ \frac{1}{n} \sum_{i=1}^n X_i \right] \\
    &= \frac{1}{n} \sum_{i=1}^{n} \mathbb{E}\left[ X_i \right] \\
    &\overset{(\ref{eq:expected_X_i})}{=} \frac{1}{n} \sum_{i=1}^{n} \frac{1}{2} \\
    &= \frac{1}{n} \cdot n \cdot \frac{1}{2} \\
    &= \frac{1}{2}
\end{align*}


Therefore, the expected value of $\mathbb{E}[ \bar{X}(N)]$ estimates the true mean of $X$.

\subsection*{(b)}
\textit{Find the $\text{Var}(\bar{X}(N))$ analytically. Hint: find the variance of a
single $X_i$ first.}

Let us first compute the exectation of $\mathbb{E}[X_i^2]$:
\begin{align*}
    \mathbb{E}[X_i^2] &= \int_{0}^{1} x^2 \cdot p(X=x) \; \dif x \\
    &= \int_{0}^{1} x^2 \cdot {\frac{1}{1-0}} \; \dif x \\
    &= \int_{0}^{1} x^2\; \dif x \\
    &= \left[ \frac{1}{3}\cdot x^3 \right]_0^1 \\
    &= \frac{1}{3}(1 - 0)\\ 
    &= \frac{1}{3} \stepandtag \label{eq:expectation_X_i_square}
\end{align*}

We can now compute the variance of a $X_i$ as follows:
\begin{align*}
    \var (X_i) &= \mathbb{E}\left[ (X_i - \mathbb{E}[X_i])^2 \right] \\
    &= \mathbb{E}\left[ (X_i - \frac{1}{2})^2 \right] \\
    &= \mathbb{E}\left[ X_i^2 + \frac{1^2}{2^2} - 2\cdot X_i \cdot \frac{1}{2} \right] \\
    &= \mathbb{E}\left[ X_i^2 + \frac{1^2}{2^2} - X_i \right] \\
    &= \mathbb{E}\left[ X_i^2 \right] + \mathbb{E}\left[\frac{1}{4} \right] - \mathbb{E} \left[X_i \right] \\
    &= \mathbb{E}\left[ X_i^2 \right] + \frac{1}{4} - \frac{1}{2} \\
    &\overset{(\ref{eq:expectation_X_i_square})}{=} \frac{1}{3} + \frac{1}{4} - \frac{1}{2} \\
    &= \frac{4}{12} + \frac{3}{12} - \frac{6}{12} \\
    &= \frac{1}{12} \; .
\end{align*}

We can now compute the variance of $\bar{X}(N)$ as follows:
\begin{align*}
    \var(\bar{X}(N)) &= \var \left( \frac{1}{N} \sum_{i=1}^N X_i \right) \\
    &= \frac{1}{N^2} \cdot \var \left( \sum_{i=1}^{N} X_i \right) \\
    &= \frac{1}{N^2} \cdot \sum_{i=1}^{N} \var(X_i) \stepandtag \label{eq:iid_assumption} \\
    &= \frac{1}{N^2} \cdot \sum_{i=1}^{N} \frac{1}{12} \\
    &= \frac{1}{N^2} \cdot N \cdot \frac{1}{12} \\
    &= \frac{1}{N} \cdot \frac{1}{12}
\end{align*}
where Equation~\ref{eq:iid_assumption} holds as each drawing $X_i$ is independent of each other drawing $X_j$ ($\Pr(X_i \mid X_j) = \Pr(X_i)$), and we can therefore use the property of the variance for i.i.d. variables $\var(X+Y) = \var(X) + \var(Y)$. 

Therefore, in the limit, we have
\begin{equation*}
    \lim_{n\rightarrow \infty} \var(\bar(X)(N)) = \lim_{n\rightarrow \infty} \frac{1}{N} \cdot \frac{1}{12} = 0
\end{equation*}
with a linear convergence rate.

\subsection*{(c)}

The probability density function of $\hat{X}(N)$ is given as 
\begin{equation*}
    f_{\hat{X}(N)}(x) = \binom{N}{\frac{N-1}{2}} \cdot \frac{N+1}{2} \cdot x^{\frac{N-1}{2}} \cdot (1-x)^{\frac{N-1}{2}} \;. 
\end{equation*}

Let us now compute the expectation of $\hat{X}(N)$:
\begin{align*}
    \mathbb{E}[\hat{X}(N)] &= \int_{0}^{1} x \cdot f_{\hat{X}(N)}(x) \dif x \\
    &= \int_{0}^{1} x \cdot \binom{N}{\frac{N-1}{2}} \cdot \frac{N+1}{2} \cdot x^{\frac{N-1}{2}} \cdot (1-x)^{\frac{N-1}{2}} \dif x \\
    &= \binom{N}{\frac{N-1}{2}} \cdot \frac{N+1}{2} \cdot \int_{0}^{1} x \cdot x^{\frac{N-1}{2}} \cdot (1-x)^{\frac{N-1}{2}} \dif x \\
    &= \binom{N}{\frac{N-1}{2}} \cdot \frac{N+1}{2} \cdot \int_{0}^{1} x^{\frac{N-1}{2}+1} \cdot (1-x)^{\frac{N-1}{2}} \dif x \\
    &= \binom{N}{\frac{N-1}{2}} \cdot \frac{N+1}{2} \cdot \int_{0}^{1} x^{\frac{N+1}{2}} \cdot (1-x)^{\frac{N-1}{2}} \dif x \\
% \end{align*}
% where we use integration by parts to compute the integral. Let $u(x) = (1-x)^{\frac{N-1}{2}}$ and $v'(x) = x^{\frac{N+1}{2}}$. Then, we can compute the integral $\int_{a}^{b} u(x) \cdot v'(x) \dif x = \left[ u(x) \cdot v(x) \right]_a^b - \int_{a}^{b} u'(x) \cdot v(x) \dif x$. Naturally, we have 
% \begin{align*}
%     v(x) &= \int v'(x) \dif x \\
%     &= \int x^{\frac{N+1}{2}} \dif x\\
%     &= x^{\frac{N+1}{2}+1} \cdot \frac{1}{\frac{N+1}{2}+1} \\
%     &= x^{\frac{N+1}{2}+1} \cdot \frac{2}{N+3} \;.
% \end{align*}
% We have
% \begin{align*}
%     u'(x) &= u(x)\; \frac{\partial}{\partial x} \\
%     &= (1-x)^{\frac{N-1}{2}} \; \frac{\partial}{\partial x} \\
%     &= f(g(x))  \; \frac{\partial}{\partial x} & \begin{cases}
%         f(x) = x^{\frac{N-1}{2}} \\
%         g(x) = 1-x
%     \end{cases}  \\
%     &= f'(g(x)) \cdot g'(x)
% \end{align*}
% with 
% \begin{align*}
%     f'(x) &= f(x) \frac{\partial}{\partial x} \\
%     &= x^{\frac{N-1}{2}} \frac{\partial}{\partial x} \\
%     &= \left( \frac{N-1}{2} \right)\cdot x^{\frac{N-1}{2}-1} \\
%     &= \left( \frac{N-1}{2} \right)\cdot x^{\frac{N-3}{2}} \\
% \end{align*}
% and
% \begin{align*}
%     g'(x) &= g(x) \frac{\partial}{\partial x} \\
%     &= 1-x \frac{\partial}{\partial x} \\
%     &= -1\;,
% \end{align*}
% thus,
% \begin{align*}
%     u'(x) &= f'(g(x)) \cdot g'(x) \\
%     &= \left( \frac{N-1}{2} \right)\cdot g(x)^{\frac{N-3}{2}} \cdot (-1) \\
%     &= \left( \frac{N-1}{2} \right)\cdot (1-x)^{\frac{N-3}{2}} \cdot (-1) \;.
% \end{align*}
% 
% Finally, we can combine everything and we have
% \begin{align*}
%     \mathbb{E}[\hat{X}(N)] &= \binom{N}{\frac{N-1}{2}} \cdot \frac{N+1}{2} \cdot \int_{0}^{1} u(x) \cdot v'(x) \dif x \\
%     &= \binom{N}{\frac{N-1}{2}} \cdot \frac{N+1}{2} \cdot \left[ u(x) \cdot v(x) \right]_0^1 - \int_{0}^{1} u'(x) \cdot v(x) \dif x \\
%     &= \binom{N}{\frac{N-1}{2}} \cdot \frac{N+1}{2} \cdot \left[ \left( (1-x)^{\frac{N-1}{2}} \right) \cdot \left( x^{\frac{N+1}{2}+1} \cdot \frac{2}{N+3} \right) \right]_0^1 - \int_{0}^{1} u'(x) \cdot v(x) \dif x \\
%     &= \binom{N}{\frac{N-1}{2}} \cdot \frac{N+1}{2} \cdot \left( 0 - 1\cdot 1\cdot \frac{2}{N+3} \right) - \int_{0}^{1} u'(x) \cdot v(x) \dif x \\
%     &= \binom{N}{\frac{N-1}{2}} \cdot \frac{N+1}{2} \cdot \left( 0 - 1\cdot 1\cdot \frac{2}{N+3} \right) - \int_{0}^{1} u'(x) \cdot v(x) \dif x \\
%     &= \binom{N}{\frac{N-1}{2}} \cdot \frac{N+1}{2} \cdot \left( -\frac{2}{N+1}-\frac{2}{2} \right) - \int_{0}^{1} u'(x) \cdot v(x) \dif x \\
% \end{align*}
% 
% \begin{align*}
%     \int_{0}^{1} u'(x) \cdot v(x) \dif x &= \int_{0}^{1} \left( \frac{N-1}{2} \right)\cdot (1-x)^{\frac{N-3}{2}} \cdot (-1) \cdot x^{\frac{N+1}{2}+1} \cdot \frac{2}{N+3} \dif x \\
%     &= - \left( \frac{N-1}{2} \right)  \cdot \frac{2}{N+3} \cdot  \int_{0}^{1} (1-x)^{\frac{N-3}{2}} \cdot x^{\frac{N+1}{2}+1} \dif x 
% \end{align*}
% 
% \begin{align*}
    % \mathbb{E}[\hat{X}(N)] &= \binom{N}{\frac{N-1}{2}} \cdot \frac{N+1}{2} \cdot \int_{0}^{1} x^{\frac{N+1}{2}} \cdot (1-x)^{\frac{N-1}{2}} \dif x \\ 
    &= \binom{N}{\frac{N-1}{2}} \cdot \frac{N+1}{2} \cdot \frac{\frac{N+1}{2}! \frac{N-1}{2}!}{\left( \frac{N+1}{2} + \frac{N-1}{2} + 1 \right)!} \stepandtag \label{eq:given_integral_result} \\
    &= \binom{N}{\frac{N-1}{2}} \cdot \frac{N+1}{2} \cdot \frac{\frac{N+1}{2}! \frac{N-1}{2}!}{\left( \frac{N-1}{2} + \frac{N-1}{2} + 1 + 1 \right)!} \\
    &= \binom{N}{\frac{N-1}{2}} \cdot \frac{N+1}{2} \cdot \frac{\frac{N+1}{2}! \frac{N-1}{2}!}{\left( 2\cdot \frac{N-1}{2} + 2 \right)!} \\
    &= \binom{N}{\frac{N-1}{2}} \cdot \frac{N+1}{2} \cdot \frac{\frac{N+1}{2}! \frac{N-1}{2}!}{\left({N-1} + 2 \right)!} \\
    &= \binom{N}{\frac{N-1}{2}} \cdot \frac{N+1}{2} \cdot \frac{\frac{N+1}{2}! \frac{N-1}{2}!}{\left(N+1 \right)!} \\
    &= \binom{N}{\frac{N-1}{2}} \cdot \frac{1}{2} \cdot \frac{\left( \frac{N+1}{2} \right)! \frac{N-1}{2}!}{N!} \\
    &= \frac{N!}{\left(\frac{N-1}{2}  \right)! \left( N-\frac{N-1}{2} \right)!} \cdot \frac{1}{2} \cdot \frac{\left( \frac{N+1}{2} \right)! \frac{N-1}{2}!}{N!} \\
    &= \frac{1}{\left(\frac{N-1}{2}  \right)! \left( N-\frac{N-1}{2} \right)!} \cdot \frac{1}{2} \cdot \frac{\left( \frac{N+1}{2} \right)! \frac{N-1}{2}!}{1} \\
    &= \frac{1}{ \left( N-\frac{N-1}{2} \right)!} \cdot \frac{1}{2} \cdot \frac{\left( \frac{N+1}{2} \right)!}{1} \\
    &= \frac{1}{ \left( \frac{N+1}{2} \right)!} \cdot \frac{1}{2} \cdot \frac{\left( \frac{N+1}{2} \right)!}{1} \stepandtag \label{eq:another_derivation} \\
    &= \frac{1}{2}
\end{align*}
where we used the given known integral rule to obtain Equation~\ref{eq:given_integral_result}, and Equation~\ref{eq:another_derivation} results from 
\begin{align*}
    N - \frac{N-1}{2} &= \frac{2 N}{2} - \frac{N-1}{2} \\
    &= \frac{N + N - (N-1)}{2} \\
    &= \frac{N + 1}{2} \;.
\end{align*}

Therefore, the exectation of the of $\hat{X}(N)$ is $\frac{1}{2}$, i.e., the true mean of $X$.

\subsection*{(d)}
\textit{Find the $\var(\hat{X}(N))$ analytically. Which distribution has the
better variance $\bar{X}(N)$ or $\hat{X}(N)$?}

We compute the expectation of $\hat{X}(N)^2$:
\begin{align*}
    \mathbb{E}[\hat{X}(N)^2] &= \int_{0}^{1} x^2 \cdot f_{\hat{X}(N)}(x) \dif x \\
    &= \binom{N}{\frac{N-1}{2}} \cdot \frac{N+1}{2} \cdot \int_{0}^{1} x^2 \cdot x^{\frac{N-1}{2}} \cdot (1-x)^{\frac{N-1}{2}} \dif x \\
    &= \binom{N}{\frac{N-1}{2}} \cdot \frac{N+1}{2} \cdot \int_{0}^{1} x^{\frac{N-1}{2}+2} \cdot (1-x)^{\frac{N-1}{2}} \dif x \\
    &= \binom{N}{\frac{N-1}{2}} \cdot \frac{N+1}{2} \cdot \int_{0}^{1} x^{\frac{N+3}{2}} \cdot (1-x)^{\frac{N-1}{2}} \dif x \\
    &= \binom{N}{\frac{N-1}{2}} \cdot \frac{N+1}{2} \cdot \frac{\frac{N+3}{2}! \frac{N-1}{2}!}{\left( \frac{N+3}{2} + \frac{N-1}{2} + 1 \right)!} \\
    &= \binom{N}{\frac{N-1}{2}} \cdot \frac{N+1}{2} \cdot \frac{\frac{N+3}{2}! \frac{N-1}{2}!}{\left( \frac{N-1}{2} + \frac{N-1}{2} + 1 + 2 \right)!} \\
    &= \binom{N}{\frac{N-1}{2}} \cdot \frac{N+1}{2} \cdot \frac{\frac{N+3}{2}! \frac{N-1}{2}!}{\left( 2\cdot\frac{N-1}{2} + 3 \right)!} \\
    &= \binom{N}{\frac{N-1}{2}} \cdot \frac{N+1}{2} \cdot \frac{\frac{N+3}{2}! \frac{N-1}{2}!}{\left( N-1 + 3 \right)!} \\
    &= \binom{N}{\frac{N-1}{2}} \cdot \frac{N+1}{2} \cdot \frac{\frac{N+3}{2}! \frac{N-1}{2}!}{\left( N+2 \right)!} \\
    &= \binom{N}{\frac{N-1}{2}} \cdot \frac{N+1}{2} \cdot \frac{\frac{N+3}{2}! \frac{N-1}{2}!}{(N+2)\cdot\left( N+1 \right)!} \\
    &= \binom{N}{\frac{N-1}{2}} \cdot \frac{1}{2} \cdot \frac{\frac{N+3}{2}! \frac{N-1}{2}!}{(N+2)\cdot N !} \\
    &= \frac{N!}{\left(\frac{N-1}{2}  \right)! \left( N-\frac{N-1}{2} \right)!} \cdot \frac{1}{2} \cdot \frac{\frac{N+3}{2}! \frac{N-1}{2}!}{(N+2)\cdot N !} \\
    &= \frac{1}{\left(\frac{N-1}{2}  \right)! \left( N-\frac{N-1}{2} \right)!} \cdot \frac{1}{2} \cdot \frac{\frac{N+3}{2}! \frac{N-1}{2}!}{(N+2)} \\
    &= \frac{1}{\left( N-\frac{N-1}{2} \right)!} \cdot \frac{1}{2} \cdot \frac{\frac{N+3}{2}!}{(N+2)} \\
    &= \frac{1}{\frac{N+1}{2}!} \cdot \frac{1}{2} \cdot \frac{\frac{N+3}{2}!}{(N+2)} \\
    &= \frac{1}{\frac{N+1}{2}!} \cdot \frac{1}{2} \cdot \frac{\frac{N+3}{2}\cdot \left(  \frac{N+1}{2}\right)!}{(N+2)} \\
    &= \frac{1}{2} \cdot \frac{\frac{N+3}{2}}{(N+2)} \\
    &= \frac{1}{2} \cdot \frac{\frac{N+3}{2} \cdot 2}{(N+2) \cdot 2} \\
    &= \frac{1}{2} \cdot \frac{N+3}{(N+2) \cdot 2} \\
    &= \frac{1}{2} \cdot \frac{N+2 + 1}{(N+2) \cdot 2} \\
    &= \frac{1}{2} \cdot \left( \frac{N+2}{(N+2) \cdot 2} + \frac{1}{(N+2) \cdot 2}  \right) \\
\end{align*}

\begin{align*}
    &= \frac{1}{2} \cdot \left( \frac{1}{2} + \frac{1}{2N+4}  \right) \\
    &= \frac{1}{4} + \frac{1}{4N+8} \\
\end{align*}
     
We can finally compute the variance of $\hat{X}(N)$ as follows:
\begin{align*}
    \var(\hat{X}(N)) &= \mathbb{E}[\hat{X}(N)^2] - \mathbb{E}[\hat{X}(N)]^2 \\
    &= \mathbb{E}[\hat{X}(N)^2] - \left( \frac{1}{2} \right)^2 \\
    &= \mathbb{E}[\hat{X}(N)^2] - \frac{1}{4} \\
    &= \frac{1}{4} + \frac{1}{4N+8} - \frac{1}{4} \\
    &= \frac{1}{4N+8}
\end{align*}
which, in the limit, converges to zero:
\begin{align*}
    \lim_{N\rightarrow \infty} \var(\hat{X}(N)) = \lim_{N\rightarrow \infty} \frac{1}{4N+8} = 0
\end{align*}

As the convergence rate of $\var(\hat{X}(N))$ is slower than $\var(\bar{X}(N))$, and that both have the same (correct) expectation, we can conclude that $\bar{X}(N)$ has the better variance, i.e., it is better to estimate the mean of a uniform distribution by using the mean of the samples instead of the median.

\subsection*{(e)}
See provided code.

\pagebreak
\section*{Problem 2}

\subsection*{(a)}
\textit{Show that if $E < 0$, then the game is losing in a long run and if $E > 0$, then the game is winning in a long run.}

Let $\pi$ be the stationary distribution of the Markov chain, and 
\begin{equation*}
    E = \pi_0 \cdot (B1_+ - B1_-) + (\pi_1 + \pi_2) \cdot (B2_+ - B2_-) \;.
\end{equation*}

We remark that $E$ is composed of two sums, the first sum (which includes $\pi_0$) denotes the expected return when in state $0$. As $\pi$ is the stationary distribution, the state $0$ is reached with $\pi_0$ probability at each time step in the long run. The $(B1_+ - B1_-)$ part is positive if probability of gaining one dollar is greater than losing one dollar when playing game $B$ while in state $0$.\!\footnote{See the definition of the states in (b).}

Because the second part of the equation follows the same structure (one could multiply out $(\pi_1 + \pi_2)$ to obtain the same argument for the two remaning states of the Markov chain), we can conclude that $E$ denotes the expected \emph{gain} (or \emph{return}) when playing game B in the long run.

Therefore, if the expected gain is negative, the game is losing in the long run. On the other hand, a positive expected gain would imply that the game is winning.



\subsection*{(b)}

We three states in our Markov chain: $0$, $1$, and $2$, which denotes the number that results from the current capital modulo 3.
We now define the transition matrix $P$ as follows:
\begin{equation*}
    P = \begin{pmatrix}
        0.0 & 0.095 & 0.905  \\ 
        0.255 & 0.0 & 0.745 \\
        0.745 & 0.255 & 0.0
    \end{pmatrix}
\end{equation*}

We remark that each play results in a win or a loss, and therefore a probability of $0.0$ to stay in the same state after playing a round, hence, the diagonal elements of $P$ are all $0.0$.

The stationary distribution of the Markov chain is denoted as 
\begin{equation*}
    \pi := \begin{pmatrix}
        \pi_0 &
        \pi_1 & 
        \pi_2
    \end{pmatrix} \;.
\end{equation*}

We now compute the stationary distribution from $P$ by computing the stationary value for each $\pi_i$.
\begin{align*}
    \pi_0 &= \pi_0 \cdot 0.0 + \pi_1 \cdot 0.255 + \pi_2 \cdot 0.745\\
    &= \pi_1 \cdot 0.255 + \pi_2 \cdot 0.745 \stepandtag \label{eq:pi_0}
\end{align*}

\begin{align*}
    \pi_1 &= \pi_0 \cdot 0.095 + \pi_1 \cdot 0.0 + \pi_2 \cdot 0.255\\
    \pi_1 &= \pi_0 \cdot 0.095 + \pi_2 \cdot 0.255\\
    \pi_1 &\overset{(\ref{eq:pi_0})}{=} \left( \pi_1 \cdot 0.255 + \pi_2 \cdot 0.745 \right) \cdot 0.095 + \pi_2 \cdot 0.255\\
    \pi_1 &=  \pi_1 \cdot 0.255 \cdot 0.095 + \pi_2 \cdot 0.745 \cdot 0.095 + \pi_2 \cdot 0.255 &&\mid -\left( \pi_1 \cdot 0.255 \cdot 0.095 \right) \\
    \pi_1 (1 - 0.024225) &= \pi_2 \cdot 0.745 \cdot 0.095 + \pi_2 \cdot 0.255 && \mid \div (1 - 0.024225) \\
    \pi_1 &= \frac{\pi_2 \cdot \left( 0.255 + 0.070775 \right)}{(1 - 0.024225)}  \\
    \pi_1 &= \pi_2 \cdot  \frac{0.325775}{0.975775}  \\
    \pi_1 &= \pi_2 \cdot  0.333862827  \stepandtag \label{eq:pi_1} \\
\end{align*}


Using the fact that $\pi_0 + \pi_1 + \pi_2 = 1$, we can now compute $\pi_2$:
\begin{align*}
    1 &= \pi_0 + \pi_1 + \pi_2 \\
    1 &\overset{(\ref{eq:pi_0})}{=} \left( \pi_1 \cdot 0.255 + \pi_2 \cdot 0.745 \right) + \pi_1 + \pi_2 \\
    1 &= \pi_1 \cdot 1.255 + \pi_2 \cdot 1.745 \\
    1 &\overset{(\ref{eq:pi_1})}{=} \left( \pi_2 \cdot  0.333862827 \right) \cdot 1.255 + \pi_2 \cdot 1.745 \\
    1 &= \pi_2 \cdot  0.4189978479 + \pi_2 \cdot 1.745 \\
    1 &= \pi_2 \cdot  2.1639978479 \\
    \pi_2 &= \frac{1}{2.1639978479} \\
    \pi_2&= 0.4621076684
\end{align*}
from which we can infer 
\begin{align*}
    \pi_1 &= \pi_2 \cdot  0.333862827 \\ &= 0.4621076684 \cdot 0.333862827 \\ &= 0.1542805726
\end{align*}
and
\begin{align*}
    \pi_0 &= \pi_1 \cdot 0.255 + \pi_2 \cdot 0.745 \\
    &= 0.1542805726 \cdot 0.255 + 0.4621076684 \cdot 0.745 \\
    &= 0.03934154601 + 0.344270213 \\
    &= 0.383611759 \;.
\end{align*}

Therefore, we have the stationary distribution
\begin{equation*}
    \pi = \begin{pmatrix}
        0.383611759 &
        0.1542805726 &
        0.4621076684
    \end{pmatrix}
\end{equation*}
with $\pi = \pi P$ and total sum of one ($0.383611759 + 0.1542805726 + 0.4621076684 = 1$).

Therefore, the expected gain when playing game B in the long run is 
\begin{align*}
    E &= \pi_0 \cdot (B1_+ - B1_-) + (\pi_1 + \pi_2) \cdot (B2_+ - B2_-) \\
    &= \pi_0 \cdot (0.095 - 0.905) + (\pi_1 + \pi_2) \cdot (0.745 - 0.255) \\
    &= 0.383611759 \cdot (-0.81) + (0.1542805726 + 0.4621076684) \cdot 0.49 \\
    &= 0.383611759 \cdot (-0.81) + 0.616388241 \cdot 0.49 \\
    % &= 0.383611759 \cdot (0.095 - 0.905) + (0.1542805726 + 0.4621076684) \cdot (0.745 - 0.255) \\
    &= -0.0086952867
\end{align*}

\subsection*{(c)}
See provided code.

\subsection*{(d)}

% We extend the Markov chain of (b) by adding three extra states: $3, 4$, and $5$, which denotes again the current capital module 3. We introduce the three new states for representing the states of game A, whereas the current three states ($0, 1, 2$) represent the states of game B.
We first observe that we can keep the same three states of the Markov chain as introduced in (b). However, we need to update the transition matrix $P$.
We now construct $P$ to model the transition between the states given that the player \textit{plays the random game with equal probabilities every time}, i.e., plays game A with probability $0.5$ and game B with probability $0.5$. Let us denote the transition matrix of game B as introduced in (b) as $P_B$. We now construct the transition matrix $P_A$ for game A as follows:
\begin{equation*}
    P_A = \begin{pmatrix}
        0.0 & 0.495 & 0.505 \\
        0.505 & 0.0 & 0.495 \\
        0.495 & 0.505 & 0.0
    \end{pmatrix}
\end{equation*}
where in each state, the probability of winning one dollar is $0.495$ and the probability of losing one dollar is $0.505$. We can now construct the transition matrix for the combined Markov chain, i.e., when playing both games with equal probabilities, as follows:
\begin{align*}
    P &= 0.5 \cdot P_A + 0.5 \cdot P_B \\
    &= \begin{pmatrix}
        0.0   & 0.295 & 0.705 \\
        0.38  & 0.0   & 0.62  \\
        0.62  & 0.38  & 0.0  
    \end{pmatrix}\;.
\end{align*}

We can now compute the stationary distribution of the new Markov chain as done in (b). In the following derivation, we may skip some steps for brevity, as the exact same derivation process as in (b) is followed.

We start by computing $\pi_0$ as
\begin{equation}
    \pi_0 = \pi_1 \cdot 0.38 + \pi_2 \cdot 0.62
\end{equation}
and 
\begin{align*}
    \pi_1 &= \pi_0 \cdot 0.295 + \pi_2 \cdot 0.38 \\
    \pi_1 &= \left( \pi_1 \cdot 0.38 + \pi_2 \cdot 0.62 \right) \cdot 0.295 + \pi_2 \cdot 0.38 \\
    \pi_1 &= \pi_1 \cdot 0.1121 + \pi_2 \cdot 0.1829 + \pi_2 \cdot 0.38 \\
    % \pi_1 \cdot(1 - 0.1121) &= + \pi_2 \cdot 0.5629 \\
    \pi_1 &= \pi_2 \cdot 0.6339677892 \;.
\end{align*}

Thus, we can compute $\pi_2$:
\begin{align*}
    1 &= \pi_0 + \pi_1 + \pi_2 \\
    1 &= \left( \pi_1 \cdot 0.38 + \pi_2 \cdot 0.62 \right) + \pi_1 + \pi_2 \\
    1 &= \pi_1 \cdot 1.38 + \pi_2 \cdot 1.62  \\
    1 &= \pi_2 \cdot 0.6339677892 \cdot 1.38 + \pi_2 \cdot 1.62  \\
    1 &= \pi_2 \cdot 2.4948755491  \\
    \pi_2 &= \frac{1}{2.4948755491} \\
    \pi_2 &= 0.4008215962
\end{align*}
and infer $\pi_0$ and $\pi_1$:
\begin{align*}
    \pi_1 &= \pi_2 \cdot 0.6339677892 \\
    &= 0.2541079812
\end{align*}
and 
\begin{align*}
    \pi_0 &= \pi_1 \cdot 0.38 + \pi_2 \cdot 0.62 \\
    &= 0.2541079812 \cdot 0.38 + 0.4008215962 \cdot 0.62 \\
    &= 0.3450704225
\end{align*}

Therefore, we can now compute the expected gain when playing the new game by first extracting the probabilities of winning and losing in each state. Let us denote the state $0, 1, 2$ as $A, B, C$ to ease the notation. We have 
\begin{gather*}
    A_+ = 0.295 \\
    A_- = 0.705 \\
    B_+ = 0.62 \\
    B_- = 0.38 \\
    C_+ = 0.62 \\
    C_- = 0.38
\end{gather*}
and can finally compute the expected gain of the new Markov chain as follows:\!\footnote{We could also summarize Equation~\ref{eq:new_formular} by clusting $\pi_1$ and $\pi_2$ into a single term as done in the assignment sheet. However, the formulation in Equation~\ref{eq:new_formular} makes the derivation more transparent.}
\begin{align*}
    E &= \pi_0 \cdot (A_+ - A_-) + \pi_1 \cdot (B_+ - B_-) + \pi_2 \cdot (C_+ - C_-) \stepandtag \label{eq:new_formular} \\
    &= \pi_0 \cdot (0.295 - 0.705) + \pi_1 \cdot (0.62 - 0.38) + \pi_2 \cdot (0.62 - 0.38) \\
    &= 0.3450704225 \cdot (-0.41) +  0.2541079812 \cdot 0.24 + 0.4008215962 \cdot 0.24 \\
    &= 0.01570422535
\end{align*}

Therefore, the expected gain when playing the new game is positive, indicating that, on average, the player will win in the long run.

\subsection*{(e)}
See provided code.

\subsection*{(f)}
\textit{Show analytically that the provided periodic strategy is winning in a long run.}

We wish to investigate whether playing both games in a given sequence (ABBABBABB...) is also winning.

We observe that the sequence is a repeating pattern of playing game A followed by playing game B two times.
We can therefore compute the transition matrix for one of such patterns as follows:
\begin{equation*}
    P_{ABB} = P_A \cdot P_B \cdot P_B = P_A \cdot P_B^2 \;.
\end{equation*}
Note that the complete transition matrix of the Markov chain $P = (P_{ABB})^\infty$, which is equivalent to playing the game in a repeating pattern of ABBABBABB... in the long run. To this end, we will compute the stationary distribution of $P_{ABB}$ which will indicate whether playing the game in this pattern is winning.

We therefore use $P_A$ and $P_B$ as previously introduced in (b) and (d) and compute $P_B^2$:
\begin{align*}
    P_B^2 &= P_B \cdot P_B \\
    &= \begin{pmatrix}
        0.69845 & 0.230775 & 0.070775 \\
        0.555025 & 0.2142 & 0.230775 \\
        0.065025 & 0.070775 & 0.8642
    \end{pmatrix}\;.
\end{align*}
Finally, we can compute $P_{ABB}$:
\begin{align*}
    P_{ABB} &= P_A \cdot P_B^2 \\
    &= \begin{pmatrix}
        0.307575 & 0.14177038 & 0.55065462 \\
        0.38490463 & 0.151575 & 0.46352037 \\
        0.62602038 & 0.22240463 & 0.151575
    \end{pmatrix}\;.
\end{align*}

We now compute the stationary distribution $\pi$ of $P_{ABB}$. We, again, reuse the same derivation process as in (b) and may therefore skip some steps for brevity.

\begin{align*}
    \pi_0 &= \pi_0 \cdot 0.307575 + \pi_1 \cdot 0.38490463 + \pi_2 \cdot 0.62602038 \\
    \pi_0 \cdot (1 - 0.307575) &= \pi_1 \cdot 0.38490463 + \pi_2 \cdot 0.62602038 \\
    \pi_0 &= \pi_1 \cdot \frac{0.38490463}{0.692425} + \pi_2 \cdot \frac{0.62602038}{0.692425} \\
    \pi_0 &= \pi_1 \cdot 0.5558791638 + \pi_2 \cdot 0.9040984655 \stepandtag \label{eq:pi_0_ABB} \\
\end{align*}
and 
\begin{align*}
    \pi_1 &= \pi_0 \cdot 0.14177038 + \pi_1 \cdot 0.151575 + \pi_2 \cdot 0.22240463 \\
    \pi_1 \cdot (1- 0.151575) &= \pi_0 \cdot 0.14177038 + \pi_2 \cdot 0.22240463 \\
    \pi_1 &= \pi_0 \cdot 0.1670983057 + \pi_2 \cdot 0.2621382326 \\
    \pi_1 &\overset{(\ref{eq:pi_0_ABB})}{=} \left( \pi_1 \cdot 0.5558791638 + \pi_2 \cdot 0.9040984655 \right) \cdot 0.1670983057 + \pi_2 \cdot 0.2621382326 \\
    \pi_1 &= \pi_2 \cdot 0.455523525 \;.
\end{align*}

Therefore, we have 
\begin{align*}
    1 &= \pi_0 + \pi_1 + \pi_2 \\
    1 &= \left( \pi_1 \cdot 0.5558791638 + \pi_2 \cdot 0.9040984655 \right) + \pi_1 + \pi_2 \\
    1 &= \pi_1 \cdot 1.5558791638 + \pi_2 \cdot 1.9040984655 \\
    1 &= \pi_2 \cdot 0.455523525 \cdot 1.5558791638 + \pi_2 \cdot 1.9040984655 \\
    1 &= \pi_2 \cdot 2.6128380267 \\
    \pi_2 &= \frac{1}{2.6128380267} \\
    \pi_2&= 0.3827255994 \;.
\end{align*}
Furthermore, 
\begin{align*}
    \pi_1 &= \pi_2 \cdot 0.455523525\\
    &= 0.3827255994 \cdot 0.455523525 \\
    &= 0.1743405141
\end{align*}
and 
\begin{align*}
    \pi_0 &= \pi_1 \cdot 0.5558791638 + \pi_2 \cdot 0.9040984655 \\
    &= 0.1743405141 \cdot 0.5558791638 + 0.3827255994 \cdot 0.9040984655 \\
    &= 0.4429338863\;.
\end{align*}

Lastly, we compute the expected gain when playing this repeating pattern in the long run.
As $P$ now represents three rounds instead of one, we need to compute the values for $A_-, A_+, \dots$.
% Note that we can ignore the transitions where we stay in the same state, and only consider gains and losses when transitioning to a higher or lower state.
% \begin{align*}
%     E &= \pi_0 \cdot (A_+ - A_-) + \pi_1 \cdot (B_+ - B_-) + \pi_2 \cdot (C_+ - C_-) \\
%     &= \pi_0 \cdot (0.14177038 - 0.55065462) + \pi_1 \cdot (0.46352037 - 0.38490463) + \pi_2 \cdot (0.62602038 - 0.22240463) \\
%     &= \pi_0 \cdot (-0.40888424) + \pi_1 \cdot 0.07861574 + \pi_2 \cdot 0.40361575 \\
%     &= 0.4429338863 \cdot (-0.40888424) + 0.1743405141 \cdot 0.07861574 + 0.3827255994 \cdot 0.40361575 \\
%     &= 0.4429338863 \cdot (-0.40888424) + 0.1743405141 \cdot 0.07861574 + 0.3827255994 \cdot 0.40361575 \\
%     &= -0.0129286971\;.
% \end{align*}
% This indicates that playing the game in this repeating patterin is losing in the long run.


From this, we can compute the expected gain for a playing a round of game A in the long run. Note that we can read $A_+, A_-, \dots$ directly from $P_A$.
\begin{align*}
    E^{(A)} &= \pi_0 \cdot (A_+ - A_-) + \pi_1 \cdot (B_+ - B_-) + \pi_2 \cdot (C_+ - C_-) \\
    &= \pi_0 \cdot (0.495 - 0.505) + \pi_1 \cdot (0.495 - 0.505) + \pi_2 \cdot (0.495 - 0.505) \\
    &= (0.495 - 0.505) \cdot \left( \pi_0 + \pi_1 + \pi_2 \right) \\
    &= (-0.01) \cdot 1\\ &= -0.01\;.
\end{align*}

For computing the average reward for playing the first round of game B after game A,  we compute the probability of being in each state after having played game A:
\begin{align*}
    \pi^{(A)} &= \pi \cdot P_A \\
    &= \begin{pmatrix}
        0.4429338863 & 0.1743405141 & 0.3827255994
    \end{pmatrix} \cdot \begin{pmatrix}
        0.0 & 0.495 & 0.505 \\
        0.505 & 0.0 & 0.495 \\
        0.495 & 0.505 & 0.0
    \end{pmatrix} \\
    &= \begin{pmatrix}
        0.27749113 & 0.4125287 & 0.30998017
    \end{pmatrix}\;.
\end{align*}

The expected gain is 
\begin{align*}
    E^{(AB)} &= \pi^{(A)}_0 \cdot (A_+ - A_-) + \pi_1^{(A)} \cdot (B_+ - B_-) + \pi_2^{(A)} \cdot (C_+ - C_-) \\
    &= \pi_0^{(A)} \cdot (0.095 - 0.905) + \pi_1^{(A)} \cdot (0.745 - 0.255) + \pi_2^{(A)} \cdot (0.745 - 0.255) \\
    &= 0.27749113 \cdot (-0.81) + 0.4125287 \cdot 0.49 + 0.30998017 \cdot 0.49 \\
    &= 0.129261531\;.
\end{align*}

Similarly, we can compute the expected gain for playing a round of game B after playing a round of game A.
\begin{align*}
    \pi^{(AB)} &= \pi^{(A)} \cdot P_B \\
    &= \begin{pmatrix}
        0.33613004 & 0.1054066 & 0.55846336
    \end{pmatrix}
\end{align*}
and 
% Therefore, the expectedg gain in the last second round of playing game B is
\begin{align*}
    E^{(ABB)} &= \pi^{(AB)}_0 \cdot (A_+ - A_-) + \pi_1^{(AB)} \cdot (B_+ - B_-) + \pi_2^{(AB)} \cdot (C_+ - C_-) \\
    &= 0.33613004 \cdot (-0.81) + 0.1054066 \cdot 0.49 + 0.55846336 \cdot 0.49 \\
    &= 0.053030948\;.
\end{align*} 

To total expected reward for playing all three rounds can be computed as follows:
\begin{align*}
    E &= E^{(A)} + E^{(AB)} + E^{(ABB)} \\
    &= (-0.01) + 0.129261531 + 0.053030948 \\
    &= 0.172292479\;.
\end{align*}
this shows that playing all three rounds in this specific order yields a positive expected reward, and therefore is winning in the long run.


% Finally, we compute the probability of each state after playing the second round of game B:
\textit{(Optional)}\\
We can verify that $\pi^{(AB)}$ is valid by computing the probability of each state after the second round of game B: 
\begin{align*}
    \pi^{(ABB)} &= \pi^{(AB)} \cdot P_B \\
    &= \begin{pmatrix}
        0.44293388 & 0.17434051 & 0.38272561
    \end{pmatrix}\;.
\end{align*}
We remark that $\pi^{(ABB)} = \pi$, which is expected, as we just computed $\pi \cdot P$ step by step. 

\pagebreak
\section*{Problem 4}

\subsection*{(b)}
We got given the following formula for the likelihood of observing counts with selected
model parameters as

\begin{align*}
    L(\lambda_{bg}, \lambda_{burst}) &= \prod_{i}^{} P(c_i \mid \lambda_{bg} , \lambda_{burst}) \\
    &= \prod_{i:t_i < t_{start} \text{ or } t_i > t_{end}}^{} \frac{\exp(-\lambda_{bg}) \cdot \lambda_{bg}^{c_i}}{c_i!} \cdot \prod_{i:t_{start} \leq t_i \leq t_{end}}^{} \frac{\exp(-\lambda_{burst}) \cdot \lambda_{burst}^{c_i}}{c_i!} \stepandtag \label{eq:likelihood_formula}
\end{align*}
where $c_i$ is the number of counts in bin $i$ starting at time $t_i$.

We now derive the maximum likelihood estimate for the model parameters $\lambda_{bg}$ and $\lambda_{burst}$.
\begin{align*}
    \log L(\lambda_{bg}, \lambda_{burst}) &= \sum_{i:t_i < t_{start} \text{ or } t_i > t_{end}}^{} \log \left( \frac{\exp(-\lambda_{bg})\cdot \lambda_{bg}^{c_i}}{c_i!} \right) + \sum_{i:t_{start} \leq t_i \leq t_{end}}^{} \log \left( \frac{\exp(-\lambda_{burst})\cdot \lambda_{burst}^{c_i}}{c_i!} \right) \\
    &= \sum_{i:t_i < t_{start} \text{ or } t_i > t_{end}}^{} \log \left( {\exp(-\lambda_{bg})\cdot \lambda_{bg}^{c_i}} \right) - \log \left( c_i! \right) + \sum_{i:t_{start} \leq t_i \leq t_{end}}^{} \log \left( {\exp(-\lambda_{burst})\cdot \lambda_{burst}^{c_i}} \right) - \log \left( c_i! \right) \\
    % &= \sum_{i:t_i < t_{start} \text{ or } t_i > t_{end}}^{} -\lambda_{bg} + \lambda_{bg}^{c_i} - \log \left( c_i! \right) + \sum_{i:t_{start} \leq t_i \leq t_{end}}^{} -\lambda_{burst} + \lambda_{burst}^{c_i} - \log \left( c_i! \right)
    &= \sum_{i:t_i < t_{start} \text{ or } t_i > t_{end}}^{} -\lambda_{bg} + c_i\cdot\log(\lambda_{bg}) - \log \left( c_i! \right) + \sum_{i:t_{start} \leq t_i \leq t_{end}}^{} -\lambda_{burst} + c_i\cdot\log(\lambda_{burst}) - \log \left( c_i! \right) \\
    % &= \sum_{i:t_i < t_{start} \text{ or } t_i > t_{end}}^{} \lambda_{bg} \cdot(c_i - 1) - \log \left( c_i! \right) + \sum_{i:t_{start} \leq t_i \leq t_{end}}^{} \lambda_{burst}\cdot(c_i - 1) - \log \left( c_i! \right)
\end{align*}

Which we can now use to compute the gradients w.r.t. to $\lambda$:
\begin{align*}
    \frac{\partial \log L(\lambda_{bg}, \lambda_{burst})}{\partial \lambda_{bg}} &= \sum_{i:t_i < t_{start} \text{ or } t_i > t_{end}}^{} -1 + c_i \cdot \frac{1}{\lambda_{bg}} \\
    \frac{\partial \log L(\lambda_{bg}, \lambda_{burst})}{\partial \lambda_{burst}} &= \sum_{i:t_{start} \leq t_i \leq t_{end}}^{} -1 + c_i \cdot \frac{1}{\lambda_{burst}} 
\end{align*}
thus
\begin{equation*}
    \nabla \log L(\lambda_{bg}, \lambda_{burst}) = \begin{pmatrix}
        \sum_{i:t_i < t_{start} \text{ or } t_i > t_{end}}^{} -1 + c_i \cdot \frac{1}{\lambda_{bg}} \\
        \sum_{i:t_{start} \leq t_i \leq t_{end}}^{} -1 + c_i \cdot \frac{1}{\lambda_{burst}} 
    \end{pmatrix}\;.
\end{equation*}

We can now find the stationary points of the gradients. 
% We observe that both derivatives are sums of $c_i \cdot \lambda^{c_i-1} - 1$. Therefore, we can set the gradient to zero by finding a value for $\lambda$ for which $c_i\cdot\lambda^{c_i-1}$ is centered around -1.

We use both gradient descent and Newton's method to find the stationary points for the MLE. For Newton's method, we compute the Hessian as follows:
\begin{align*}
    H &= \begin{pmatrix}
        \frac{\partial^2 \log L(\lambda_{bg}, \lambda_{burst})}{\partial {\lambda_{bg}}^2} & \frac{\partial^2 \log L(\lambda_{bg}, \lambda_{burst})}{\partial \lambda_{bg} \cdot \lambda_{burst}} \\
        \frac{\partial^2 \log L(\lambda_{bg}, \lambda_{burst})}{\partial \lambda_{burst} \cdot \lambda_{bg}} & \frac{\partial^2 \log L(\lambda_{bg}, \lambda_{burst})}{\partial {\lambda_{burst}}^2}
    \end{pmatrix} \\
    &= \begin{pmatrix}
        \frac{\partial^2 \log L(\lambda_{bg}, \lambda_{burst})}{\partial {\lambda_{bg}}^2} & 0 \\
        0 & \frac{\partial^2 \log L(\lambda_{bg}, \lambda_{burst})}{\partial {\lambda_{burst}}^2}
    \end{pmatrix} \\
    &= \begin{pmatrix}
        \sum_{i:t_i < t_{start} \text{ or } t_i > t_{end}}^{} -c_i \cdot \frac{1}{\lambda_{bg}^2} & 0 \\
        0 & \sum_{i:t_{start} \leq t_i \leq t_{end}}^{} -c_i \cdot \frac{1}{\lambda_{burst}^2}
    \end{pmatrix}\;. 
\end{align*}

Both gradient descent and Newton's method converge to the same solution, namely 
\begin{align*}
    \lambda_{bg} &= 1.955720 \\
    \lambda_{burst} &= 3.046511\;.
\end{align*}

% As finding a closed formula for the maximum likelihood estimator of $\lambda$, we use Newton's method to find values for $\lambda$ where the derivatives are zero.

% To this end, also compute the second derivatives:

% \begin{align*}
%     \frac{\partial^2 \log L(\lambda_{bg}, \lambda_{burst})}{\partial {(\lambda_{bg})}^2} &= \sum_{i:t_i < t_{start} \text{ or } t_i > t_{end}}^{} c_i\cdot(c_i-1)\cdot\lambda_{bg}^{c_i-2} \\
%     &= \sum_{i:t_i < t_{start} \text{ or } t_i > t_{end}}^{} c_i^2\cdot(-c_i)\cdot\lambda_{bg}^{c_i-2} \\
%     \frac{\partial^2 \log L(\lambda_{bg}, \lambda_{burst})}{\partial {(\lambda_{burst})}^2} &= \sum_{i:t_{start} \leq t_i \leq t_{end}}^{} c_i \cdot (c_i-1) \cdot \lambda_{burst}^{c_i-2} \\
%     &= \sum_{i:t_{start} \leq t_i \leq t_{end}}^{} c_i^2 \cdot (-c_i) \cdot \lambda_{burst}^{c_i-2} \\
%     \frac{\partial^2 \log L(\lambda_{bg}, \lambda_{burst})}{\partial \lambda_{burst} \cdot \lambda_{bg}} &= 0\;.
% \end{align*}
% Thus, the Hessian of $\log L(\lambda_{bg}, \lambda_{burst})$ is 
% \begin{equation*}
%     H = \begin{pmatrix}
%         \sum_{i:t_i < t_{start} \text{ or } t_i > t_{end}}^{} c_i^2\cdot(-c_i)\cdot\lambda_{bg}^{c_i-2} & 0 \\
%         0 & \sum_{i:t_{start} \leq t_i \leq t_{end}}^{} c_i^2 \cdot (-c_i) \cdot \lambda_{burst}^{c_i-2}
%     \end{pmatrix}
% \end{equation*}



% \begin{align*}
%     0 &= \sum_{i}^{} -1 + c_i \cdot \lambda^{c_i -1} \\
%     0 &= -\sum_{i}^{} 1 + \sum_{i}^{} c_i \cdot \lambda^{c_i -1} \\
%     \sum_{i}^{} 1 &= \sum_{i}^{} c_i \cdot \lambda^{c_i -1} 
% \end{align*}

\subsection*{(c)}

We now investigate the null hypothesis that no burst happened, i.e., $\lambda_{bg} = \lambda_{burst} = \lambda$.
To this end, we can simply replace $\lambda_{bg}$ and $\lambda_{burst}$ with $\lambda$ in Equation~\ref{eq:likelihood_formula}.
We now have
\begin{align*}
    L(\lambda) &= \prod_{i:t_i < t_{start} \text{ or } t_i > t_{end}}^{} \frac{\exp(-\lambda) \cdot \lambda^{c_i}}{c_i!} \cdot \prod_{i:t_{start} \leq t_i \leq t_{end}}^{} \frac{\exp(-\lambda) \cdot \lambda^{c_i}}{c_i!} \\
    &= \prod_{i=1}^{t_{end}} \frac{\exp(-\lambda) \cdot \lambda^{c_i}}{c_i!} \;.
\end{align*}

We now derive the maximum likelihood estimator for $\lambda$.
\begin{align*}
    \frac{\partial \log L(\lambda)}{\partial \lambda} &= \frac{\partial}{\partial \lambda} \sum_{i=1}^{t_{end}} -\lambda + c_i \cdot \log(\lambda) - \log(c_i!) \\
    &= \sum_{i=1}^{t_{end}} -1 + c_i \cdot \frac{1}{\lambda}\;.
\end{align*}

Setting the gradient to zero, we get
\begin{align*}
    \sum_{i=1}^{t_{end}} -1 + c_i \cdot \frac{1}{\lambda} &\overset{!}{=} 0 \\
    \sum_{i=1}^{t_{end}}  c_i \cdot \frac{1}{\lambda} &{=} \sum_{i=1}^{t_{end}} 1 \\
    \frac{1}{\lambda} \cdot \sum_{i=1}^{t_{end}}  c_i &= t_{end} \\
    \lambda &= \frac{\sum_{i=1}^{t_{end}}  c_i}{t_{end}} \;.
\end{align*}

And we can verify that it is a maximum by checking the second derivative:
\begin{align*}
    \frac{\partial^2 \log L(\lambda)}{\partial \lambda^2} &= \frac{\partial}{\partial \lambda} \sum_{i=1}^{t_{end}} -1 + c_i \cdot \frac{1}{\lambda} \\
    &= \sum_{i=1}^{t_{end}} -c_i \cdot \frac{1}{\lambda^2} \\
    &= -\frac{1}{\lambda^2} \cdot\sum_{i=1}^{t_{end}} c_i
\end{align*}
and by plugging the MLE into the second derivative, we get
\begin{align*}
    -\frac{1}{\lambda^2} \cdot\sum_{i=1}^{t_{end}} c_i &= -\frac{1}{\left( \frac{\sum_{i=1}^{t_{end}}  c_i}{t_{end}} \right)^2} \cdot\sum_{i=1}^{t_{end}} c_i \\
    &= -\frac{1}{\frac{\left(\sum_{i=1}^{t_{end}}  c_i \right)^2}{t_{end}^2}} \cdot\sum_{i=1}^{t_{end}} c_i \\
    &= -\frac{t_{end}^2}{\left(\sum_{i=1}^{t_{end}}  c_i \right)^2} \cdot\sum_{i=1}^{t_{end}} c_i \\
    &= -\frac{t_{end}^2}{\sum_{i=1}^{t_{end}}  c_i } \\
    &< 0\;.
\end{align*}
Therefore, the computed MLE is a maximum.

We use python to compute the MLE for $\lambda$ for the given data and obtain a value of $\lambda = 2.3075$.


\pagebreak
\section*{Statement on the Useage of Gen. AI}

I used ChatGPT to help find literature on the properties of integration, differentiation. Otherwise, all code and text has been written entirely by me.


\end{document}
