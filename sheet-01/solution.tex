\documentclass{article}
\usepackage{import}


\title{Statisitcal Learning and Stochastic Processes \\Sheet 01 }
\author{Felix CÃ©ard-Falkenberg\\7174020}

\usepackage{homework} % See homework.sty %
\usepackage{amsmath}
\usepackage{amssymb}
\newcommand*\dif{\mathop{}\mathrm{d}}
\newcommand{\stepandtag}{%
  \refstepcounter{equation}%
  \tag{\theequation}%
}
\newcommand{\var}{\ensuremath{\text{Var}}}


\begin{document}


\section*{Problem 1}
\subsection*{(a)}
\textit{Find the $\mathbb{E}[ \bar{X}(N)]$ analytically. Does it really estimate the mean
of $X$?}

We have that \begin{equation*}
    \bar{X}(N) = \frac{1}{N} \sum_{i=1}^N X_i, \quad X_i \sim \mathrm{Uniform}(0, 1)\ .
\end{equation*}

We first compute the expectation of $X_i$. We have
\begin{align*}
    \mathbb{E}\left[ X_i \right] &= \int_{-\infty}^{\infty} x \cdot p(X=x)\; \dif x \\
    &= \int_{0}^{1} x \cdot p(X=x) \dif x \\
    &= \int_{0}^{1} x \cdot \frac{1}{1-0} \dif x \\
    &= \int_{0}^{1} x \dif x \\
    &= \left[ \frac{1}{2} x^{2} \right]_0^1 \\
    &= \frac{1}{2} (1^2 - 0^2) \\
    &= \frac{1}{2} \stepandtag \label{eq:expected_X_i}
\end{align*}


Therefore, we can compute the expectation of $\bar{X}(N)$ as follows:
\begin{align*}
    \mathbb{E}[\bar{X}(N)] &= \mathbb{E}\left[ \frac{1}{n} \sum_{i=1}^n X_i \right] \\
    &= \frac{1}{n} \sum_{i=1}^{n} \mathbb{E}\left[ X_i \right] \\
    &\overset{(\ref{eq:expected_X_i})}{=} \frac{1}{n} \sum_{i=1}^{n} \frac{1}{2} \\
    &= \frac{1}{n} \cdot n \cdot \frac{1}{2} \\
    &= \frac{1}{2}
\end{align*}


Therefore, the expected value of $\mathbb{E}[ \bar{X}(N)]$ estimates the true mean of $X$.

\subsection*{(b)}
\textit{Find the $\text{Var}(\bar{X}(N))$ analytically. Hint: find the variance of a
single $X_i$ first.}

Let us first compute the exectation of $\mathbb{E}[X_i^2]$:
\begin{align*}
    \mathbb{E}[X_i^2] &= \int_{0}^{1} x^2 \cdot p(X=x) \; \dif x \\
    &= \int_{0}^{1} x^2 \cdot {\frac{1}{1-0}} \; \dif x \\
    &= \int_{0}^{1} x^2\; \dif x \\
    &= \left[ \frac{1}{3}\cdot x^3 \right]_0^1 \\
    &= \frac{1}{3}(1 - 0)\\ 
    &= \frac{1}{3} \stepandtag \label{eq:expectation_X_i_square}
\end{align*}

We can now compute the variance of a $X_i$ as follows:
\begin{align*}
    \var (X_i) &= \mathbb{E}\left[ (X_i - \mathbb{E}[X_i])^2 \right] \\
    &= \mathbb{E}\left[ (X_i - \frac{1}{2})^2 \right] \\
    &= \mathbb{E}\left[ X_i^2 + \frac{1^2}{2^2} - 2\cdot X_i \cdot \frac{1}{2} \right] \\
    &= \mathbb{E}\left[ X_i^2 + \frac{1^2}{2^2} - X_i \right] \\
    &= \mathbb{E}\left[ X_i^2 \right] + \mathbb{E}\left[\frac{1}{4} \right] - \mathbb{E} \left[X_i \right] \\
    &= \mathbb{E}\left[ X_i^2 \right] + \frac{1}{4} - \frac{1}{2} \\
    &\overset{(\ref{eq:expectation_X_i_square})}{=} \frac{1}{3} + \frac{1}{4} - \frac{1}{2} \\
    &= \frac{4}{12} + \frac{3}{12} - \frac{6}{12} \\
    &= \frac{1}{12} \; .
\end{align*}

We can now compute the variance of $\bar{X}(N)$ as follows:
\begin{align*}
    \var(\bar{X}(N)) &= \var \left( \frac{1}{N} \sum_{i=1}^N X_i \right) \\
    &= \frac{1}{N^2} \cdot \var \left( \sum_{i=1}^{N} X_i \right) \\
    &= \frac{1}{N^2} \cdot \sum_{i=1}^{N} \var(X_i) \stepandtag \label{eq:iid_assumption} \\
    &= \frac{1}{N^2} \cdot \sum_{i=1}^{N} \frac{1}{12} \\
    &= \frac{1}{N^2} \cdot N \cdot \frac{1}{12} \\
    &= \frac{1}{N} \cdot \frac{1}{12}
\end{align*}
where Equation~\ref{eq:iid_assumption} holds as each drawing $X_i$ is independent of each other drawing $X_j$ ($\Pr(X_i \mid X_j) = \Pr(X_i)$), and we can therefore use the property of the variance for i.i.d. variables $\var(X+Y) = \var(X) + \var(Y)$. 

Therefore, in the limit, we have
\begin{equation*}
    \lim_{n\rightarrow \infty} \var(\bar(X)(N)) = \lim_{n\rightarrow \infty} \frac{1}{N} \cdot \frac{1}{12} = 0
\end{equation*}
with a linear convergence rate.

\subsection*{(c)}

The probability density function of $\hat{X}(N)$ is given as 
\begin{equation*}
    f_{\hat{X}(N)}(x) = \binom{N}{\frac{N-1}{2}} \cdot \frac{N+1}{2} \cdot x^{\frac{N-1}{2}} \cdot (1-x)^{\frac{N-1}{2}} \;. 
\end{equation*}

Let us now compute the expectation of $\hat{X}(N)$:
\begin{align*}
    \mathbb{E}[\hat{X}(N)] &= \int_{0}^{1} x \cdot f_{\hat{X}(N)}(x) \dif x \\
    &= \int_{0}^{1} x \cdot \binom{N}{\frac{N-1}{2}} \cdot \frac{N+1}{2} \cdot x^{\frac{N-1}{2}} \cdot (1-x)^{\frac{N-1}{2}} \dif x \\
    &= \binom{N}{\frac{N-1}{2}} \cdot \frac{N+1}{2} \cdot \int_{0}^{1} x \cdot x^{\frac{N-1}{2}} \cdot (1-x)^{\frac{N-1}{2}} \dif x \\
    &= \binom{N}{\frac{N-1}{2}} \cdot \frac{N+1}{2} \cdot \int_{0}^{1} x^{\frac{N-1}{2}+1} \cdot (1-x)^{\frac{N-1}{2}} \dif x \\
    &= \binom{N}{\frac{N-1}{2}} \cdot \frac{N+1}{2} \cdot \int_{0}^{1} x^{\frac{N+1}{2}} \cdot (1-x)^{\frac{N-1}{2}} \dif x \\
% \end{align*}
% where we use integration by parts to compute the integral. Let $u(x) = (1-x)^{\frac{N-1}{2}}$ and $v'(x) = x^{\frac{N+1}{2}}$. Then, we can compute the integral $\int_{a}^{b} u(x) \cdot v'(x) \dif x = \left[ u(x) \cdot v(x) \right]_a^b - \int_{a}^{b} u'(x) \cdot v(x) \dif x$. Naturally, we have 
% \begin{align*}
%     v(x) &= \int v'(x) \dif x \\
%     &= \int x^{\frac{N+1}{2}} \dif x\\
%     &= x^{\frac{N+1}{2}+1} \cdot \frac{1}{\frac{N+1}{2}+1} \\
%     &= x^{\frac{N+1}{2}+1} \cdot \frac{2}{N+3} \;.
% \end{align*}
% We have
% \begin{align*}
%     u'(x) &= u(x)\; \frac{\partial}{\partial x} \\
%     &= (1-x)^{\frac{N-1}{2}} \; \frac{\partial}{\partial x} \\
%     &= f(g(x))  \; \frac{\partial}{\partial x} & \begin{cases}
%         f(x) = x^{\frac{N-1}{2}} \\
%         g(x) = 1-x
%     \end{cases}  \\
%     &= f'(g(x)) \cdot g'(x)
% \end{align*}
% with 
% \begin{align*}
%     f'(x) &= f(x) \frac{\partial}{\partial x} \\
%     &= x^{\frac{N-1}{2}} \frac{\partial}{\partial x} \\
%     &= \left( \frac{N-1}{2} \right)\cdot x^{\frac{N-1}{2}-1} \\
%     &= \left( \frac{N-1}{2} \right)\cdot x^{\frac{N-3}{2}} \\
% \end{align*}
% and
% \begin{align*}
%     g'(x) &= g(x) \frac{\partial}{\partial x} \\
%     &= 1-x \frac{\partial}{\partial x} \\
%     &= -1\;,
% \end{align*}
% thus,
% \begin{align*}
%     u'(x) &= f'(g(x)) \cdot g'(x) \\
%     &= \left( \frac{N-1}{2} \right)\cdot g(x)^{\frac{N-3}{2}} \cdot (-1) \\
%     &= \left( \frac{N-1}{2} \right)\cdot (1-x)^{\frac{N-3}{2}} \cdot (-1) \;.
% \end{align*}
% 
% Finally, we can combine everything and we have
% \begin{align*}
%     \mathbb{E}[\hat{X}(N)] &= \binom{N}{\frac{N-1}{2}} \cdot \frac{N+1}{2} \cdot \int_{0}^{1} u(x) \cdot v'(x) \dif x \\
%     &= \binom{N}{\frac{N-1}{2}} \cdot \frac{N+1}{2} \cdot \left[ u(x) \cdot v(x) \right]_0^1 - \int_{0}^{1} u'(x) \cdot v(x) \dif x \\
%     &= \binom{N}{\frac{N-1}{2}} \cdot \frac{N+1}{2} \cdot \left[ \left( (1-x)^{\frac{N-1}{2}} \right) \cdot \left( x^{\frac{N+1}{2}+1} \cdot \frac{2}{N+3} \right) \right]_0^1 - \int_{0}^{1} u'(x) \cdot v(x) \dif x \\
%     &= \binom{N}{\frac{N-1}{2}} \cdot \frac{N+1}{2} \cdot \left( 0 - 1\cdot 1\cdot \frac{2}{N+3} \right) - \int_{0}^{1} u'(x) \cdot v(x) \dif x \\
%     &= \binom{N}{\frac{N-1}{2}} \cdot \frac{N+1}{2} \cdot \left( 0 - 1\cdot 1\cdot \frac{2}{N+3} \right) - \int_{0}^{1} u'(x) \cdot v(x) \dif x \\
%     &= \binom{N}{\frac{N-1}{2}} \cdot \frac{N+1}{2} \cdot \left( -\frac{2}{N+1}-\frac{2}{2} \right) - \int_{0}^{1} u'(x) \cdot v(x) \dif x \\
% \end{align*}
% 
% \begin{align*}
%     \int_{0}^{1} u'(x) \cdot v(x) \dif x &= \int_{0}^{1} \left( \frac{N-1}{2} \right)\cdot (1-x)^{\frac{N-3}{2}} \cdot (-1) \cdot x^{\frac{N+1}{2}+1} \cdot \frac{2}{N+3} \dif x \\
%     &= - \left( \frac{N-1}{2} \right)  \cdot \frac{2}{N+3} \cdot  \int_{0}^{1} (1-x)^{\frac{N-3}{2}} \cdot x^{\frac{N+1}{2}+1} \dif x 
% \end{align*}
% 
% \begin{align*}
    % \mathbb{E}[\hat{X}(N)] &= \binom{N}{\frac{N-1}{2}} \cdot \frac{N+1}{2} \cdot \int_{0}^{1} x^{\frac{N+1}{2}} \cdot (1-x)^{\frac{N-1}{2}} \dif x \\ 
    &= \binom{N}{\frac{N-1}{2}} \cdot \frac{N+1}{2} \cdot \frac{\frac{N+1}{2}! \frac{N-1}{2}!}{\left( \frac{N+1}{2} + \frac{N-1}{2} + 1 \right)!} \stepandtag \label{eq:given_integral_result} \\
    &= \binom{N}{\frac{N-1}{2}} \cdot \frac{N+1}{2} \cdot \frac{\frac{N+1}{2}! \frac{N-1}{2}!}{\left( \frac{N-1}{2} + \frac{N-1}{2} + 1 + 1 \right)!} \\
    &= \binom{N}{\frac{N-1}{2}} \cdot \frac{N+1}{2} \cdot \frac{\frac{N+1}{2}! \frac{N-1}{2}!}{\left( 2\cdot \frac{N-1}{2} + 2 \right)!} \\
    &= \binom{N}{\frac{N-1}{2}} \cdot \frac{N+1}{2} \cdot \frac{\frac{N+1}{2}! \frac{N-1}{2}!}{\left({N-1} + 2 \right)!} \\
    &= \binom{N}{\frac{N-1}{2}} \cdot \frac{N+1}{2} \cdot \frac{\frac{N+1}{2}! \frac{N-1}{2}!}{\left(N+1 \right)!} \\
    &= \binom{N}{\frac{N-1}{2}} \cdot \frac{1}{2} \cdot \frac{\left( \frac{N+1}{2} \right)! \frac{N-1}{2}!}{N!} \\
    &= \frac{N!}{\left(\frac{N-1}{2}  \right)! \left( N-\frac{N-1}{2} \right)!} \cdot \frac{1}{2} \cdot \frac{\left( \frac{N+1}{2} \right)! \frac{N-1}{2}!}{N!} \\
    &= \frac{1}{\left(\frac{N-1}{2}  \right)! \left( N-\frac{N-1}{2} \right)!} \cdot \frac{1}{2} \cdot \frac{\left( \frac{N+1}{2} \right)! \frac{N-1}{2}!}{1} \\
    &= \frac{1}{ \left( N-\frac{N-1}{2} \right)!} \cdot \frac{1}{2} \cdot \frac{\left( \frac{N+1}{2} \right)!}{1} \\
    &= \frac{1}{ \left( \frac{N+1}{2} \right)!} \cdot \frac{1}{2} \cdot \frac{\left( \frac{N+1}{2} \right)!}{1} \stepandtag \label{eq:another_derivation} \\
    &= \frac{1}{2}
\end{align*}
where we used the given known integral rule to obtain Equation~\ref{eq:given_integral_result}, and Equation~\ref{eq:another_derivation} results from 
\begin{align*}
    N - \frac{N-1}{2} &= \frac{2 N}{2} - \frac{N-1}{2} \\
    &= \frac{N + N - (N-1)}{2} \\
    &= \frac{N + 1}{2} \;.
\end{align*}

Therefore, the exectation of the of $\hat{X}(N)$ is $\frac{1}{2}$, i.e., the true mean of $X$.

\subsection*{(d)}
\textit{Find the $\var(\hat{X}(N))$ analytically. Which distribution has the
better variance $\bar{X}(N)$ or $\hat{X}(N)$?}

We compute the expectation of $\hat{X}(N)^2$:
\begin{align*}
    \mathbb{E}[\hat{X}(N)^2] &= \int_{0}^{1} x^2 \cdot f_{\hat{X}(N)}(x) \dif x \\
    &= \binom{N}{\frac{N-1}{2}} \cdot \frac{N+1}{2} \cdot \int_{0}^{1} x^2 \cdot x^{\frac{N-1}{2}} \cdot (1-x)^{\frac{N-1}{2}} \dif x \\
    &= \binom{N}{\frac{N-1}{2}} \cdot \frac{N+1}{2} \cdot \int_{0}^{1} x^{\frac{N-1}{2}+2} \cdot (1-x)^{\frac{N-1}{2}} \dif x \\
    &= \binom{N}{\frac{N-1}{2}} \cdot \frac{N+1}{2} \cdot \int_{0}^{1} x^{\frac{N+3}{2}} \cdot (1-x)^{\frac{N-1}{2}} \dif x \\
    &= \binom{N}{\frac{N-1}{2}} \cdot \frac{N+1}{2} \cdot \frac{\frac{N+3}{2}! \frac{N-1}{2}!}{\left( \frac{N+3}{2} + \frac{N-1}{2} + 1 \right)!} \\
    &= \binom{N}{\frac{N-1}{2}} \cdot \frac{N+1}{2} \cdot \frac{\frac{N+3}{2}! \frac{N-1}{2}!}{\left( \frac{N-1}{2} + \frac{N-1}{2} + 1 + 2 \right)!} \\
    &= \binom{N}{\frac{N-1}{2}} \cdot \frac{N+1}{2} \cdot \frac{\frac{N+3}{2}! \frac{N-1}{2}!}{\left( 2\cdot\frac{N-1}{2} + 3 \right)!} \\
    &= \binom{N}{\frac{N-1}{2}} \cdot \frac{N+1}{2} \cdot \frac{\frac{N+3}{2}! \frac{N-1}{2}!}{\left( N-1 + 3 \right)!} \\
    &= \binom{N}{\frac{N-1}{2}} \cdot \frac{N+1}{2} \cdot \frac{\frac{N+3}{2}! \frac{N-1}{2}!}{\left( N+2 \right)!} \\
    &= \binom{N}{\frac{N-1}{2}} \cdot \frac{N+1}{2} \cdot \frac{\frac{N+3}{2}! \frac{N-1}{2}!}{(N+2)\cdot\left( N+1 \right)!} \\
    &= \binom{N}{\frac{N-1}{2}} \cdot \frac{1}{2} \cdot \frac{\frac{N+3}{2}! \frac{N-1}{2}!}{(N+2)\cdot N !} \\
    &= \frac{N!}{\left(\frac{N-1}{2}  \right)! \left( N-\frac{N-1}{2} \right)!} \cdot \frac{1}{2} \cdot \frac{\frac{N+3}{2}! \frac{N-1}{2}!}{(N+2)\cdot N !} \\
    &= \frac{1}{\left(\frac{N-1}{2}  \right)! \left( N-\frac{N-1}{2} \right)!} \cdot \frac{1}{2} \cdot \frac{\frac{N+3}{2}! \frac{N-1}{2}!}{(N+2)} \\
    &= \frac{1}{\left( N-\frac{N-1}{2} \right)!} \cdot \frac{1}{2} \cdot \frac{\frac{N+3}{2}!}{(N+2)} \\
    &= \frac{1}{\frac{N+1}{2}!} \cdot \frac{1}{2} \cdot \frac{\frac{N+3}{2}!}{(N+2)} \\
    &= \frac{1}{\frac{N+1}{2}!} \cdot \frac{1}{2} \cdot \frac{\frac{N+3}{2}\cdot \left(  \frac{N+1}{2}\right)!}{(N+2)} \\
    &= \frac{1}{2} \cdot \frac{\frac{N+3}{2}}{(N+2)} \\
    &= \frac{1}{2} \cdot \frac{\frac{N+3}{2} \cdot 2}{(N+2) \cdot 2} \\
    &= \frac{1}{2} \cdot \frac{N+3}{(N+2) \cdot 2} \\
    &= \frac{1}{2} \cdot \frac{N+2 + 1}{(N+2) \cdot 2} \\
    &= \frac{1}{2} \cdot \left( \frac{N+2}{(N+2) \cdot 2} + \frac{1}{(N+2) \cdot 2}  \right) \\
\end{align*}

\begin{align*}
    &= \frac{1}{2} \cdot \left( \frac{1}{2} + \frac{1}{2N+4}  \right) \\
    &= \frac{1}{4} + \frac{1}{4N+8} \\
\end{align*}
     
We can finally compute the variance of $\hat{X}(N)$ as follows:
\begin{align*}
    \var(\hat{X}(N)) &= \mathbb{E}[\hat{X}(N)^2] - \mathbb{E}[\hat{X}(N)]^2 \\
    &= \mathbb{E}[\hat{X}(N)^2] - \left( \frac{1}{2} \right)^2 \\
    &= \mathbb{E}[\hat{X}(N)^2] - \frac{1}{4} \\
    &= \frac{1}{4} + \frac{1}{4N+8} - \frac{1}{4} \\
    &= \frac{1}{4N+8}
\end{align*}
which, in the limit, converges to zero:
\begin{align*}
    \lim_{N\rightarrow \infty} \var(\hat{X}(N)) = \lim_{N\rightarrow \infty} \frac{1}{4N+8} = 0
\end{align*}

As the convergence rate of $\var(\hat{X}(N))$ is slower than $\var(\bar{X}(N))$, and that both have the same (correct) expectation, we can conclude that $\bar{X}(N)$ has the better variance, i.e., it is better to estimate the mean of a uniform distribution by using the mean of the samples instead of the median.

\subsection*{(e)}
See provided code.

\pagebreak
\section*{Problem 2}

\subsection*{(a)}
\textit{Show that if $E < 0$, then the game is losing in a long run and if $E > 0$, then the game is winning in a long run.}

Let $\pi$ be the stationary distribution of the Markov chain, and 
\begin{equation*}
    E = \pi_0 \cdot (B1_+ - B1_-) + (\pi_1 + \pi_2) \cdot (B2_+ - B2_-) \;.
\end{equation*}

We remark that $E$ is composed of two sums, the first sum (which includes $\pi_0$) denotes the expected return when in state $0$. As $\pi$ is the stationary distribution, the state $0$ is reached with $\pi_0$ probability at each time step in the long run. The $(B1_+ - B1_-)$ part is positive if probability of gaining one dollar is greater than losing one dollar when playing game $B$ while in state $0$.\!\footnote{See the definition of the states in (b).}

Because the second part of the equation follows the same structure (one could multiply out $(\pi_1 + \pi_2)$ to obtain the same argument for the two remaning states of the Markov chain), we can conclude that $E$ denotes the expected \emph{gain} (or \emph{return}) when playing game B in the long run.

Therefore, if the expected gain is negative, the game is losing in the long run. On the other hand, a positive expected gain would imply that the game is winning.



\subsection*{(b)}

We three states in our Markov chain: $0$, $1$, and $2$, which denotes the number that results from the current capital modulo 3.
We now define the transition matrix $P$ as follows:
\begin{equation*}
    P = \begin{pmatrix}
        0.0 & 0.095 & 0.905  \\ 
        0.255 & 0.0 & 0.745 \\
        0.745 & 0.255 & 0.0
    \end{pmatrix}
\end{equation*}

We remark that each play results in a win or a loss, and therefore a probability of $0.0$ to stay in the same state after playing a round, hence, the diagonal elements of $P$ are all $0.0$.

The stationary distribution of the Markov chain is denoted as 
\begin{equation*}
    \pi := \begin{pmatrix}
        \pi_0 &
        \pi_1 & 
        \pi_2
    \end{pmatrix} \;.
\end{equation*}

We now compute the stationary distribution from $P$ by computing the stationary value for each $\pi_i$.
\begin{align*}
    \pi_0 &= \pi_0 \cdot 0.0 + \pi_1 \cdot 0.255 + \pi_2 \cdot 0.745\\
    &= \pi_1 \cdot 0.255 + \pi_2 \cdot 0.745 \stepandtag \label{eq:pi_0}
\end{align*}

\begin{align*}
    \pi_1 &= \pi_0 \cdot 0.095 + \pi_1 \cdot 0.0 + \pi_2 \cdot 0.255\\
    \pi_1 &= \pi_0 \cdot 0.095 + \pi_2 \cdot 0.255\\
    \pi_1 &\overset{(\ref{eq:pi_0})}{=} \left( \pi_1 \cdot 0.255 + \pi_2 \cdot 0.745 \right) \cdot 0.095 + \pi_2 \cdot 0.255\\
    \pi_1 &=  \pi_1 \cdot 0.255 \cdot 0.095 + \pi_2 \cdot 0.745 \cdot 0.095 + \pi_2 \cdot 0.255 &&\mid -\left( \pi_1 \cdot 0.255 \cdot 0.095 \right) \\
    \pi_1 (1 - 0.024225) &= \pi_2 \cdot 0.745 \cdot 0.095 + \pi_2 \cdot 0.255 && \mid \div (1 - 0.024225) \\
    \pi_1 &= \frac{\pi_2 \cdot \left( 0.255 + 0.070775 \right)}{(1 - 0.024225)}  \\
    \pi_1 &= \pi_2 \cdot  \frac{0.325775}{0.975775}  \\
    \pi_1 &= \pi_2 \cdot  0.333862827  \stepandtag \label{eq:pi_1} \\
\end{align*}


Using the fact that $\pi_0 + \pi_1 + \pi_2 = 1$, we can now compute $\pi_2$:
\begin{align*}
    1 &= \pi_0 + \pi_1 + \pi_2 \\
    1 &\overset{(\ref{eq:pi_0})}{=} \left( \pi_1 \cdot 0.255 + \pi_2 \cdot 0.745 \right) + \pi_1 + \pi_2 \\
    1 &= \pi_1 \cdot 1.255 + \pi_2 \cdot 1.745 \\
    1 &\overset{(\ref{eq:pi_1})}{=} \left( \pi_2 \cdot  0.333862827 \right) \cdot 1.255 + \pi_2 \cdot 1.745 \\
    1 &= \pi_2 \cdot  0.4189978479 + \pi_2 \cdot 1.745 \\
    1 &= \pi_2 \cdot  2.1639978479 \\
    \pi_2 &= \frac{1}{2.1639978479} \\
    \pi_2&= 0.4621076684
\end{align*}
from which we can infer 
\begin{align*}
    \pi_1 &= \pi_2 \cdot  0.333862827 \\ &= 0.4621076684 \cdot 0.333862827 \\ &= 0.1542805726
\end{align*}
and
\begin{align*}
    \pi_0 &= \pi_1 \cdot 0.255 + \pi_2 \cdot 0.745 \\
    &= 0.1542805726 \cdot 0.255 + 0.4621076684 \cdot 0.745 \\
    &= 0.03934154601 + 0.344270213 \\
    &= 0.383611759 \;.
\end{align*}

Therefore, we have the stationary distribution
\begin{equation*}
    \pi = \begin{pmatrix}
        0.383611759 &
        0.1542805726 &
        0.4621076684
    \end{pmatrix}
\end{equation*}
with $\pi = \pi P$ and total sum of one ($0.383611759 + 0.1542805726 + 0.4621076684 = 1$).

Therefore, the expected gain when playing game B in the long run is 
\begin{align*}
    E &= \pi_0 \cdot (B1_+ - B1_-) + (\pi_1 + \pi_2) \cdot (B2_+ - B2_-) \\
    &= \pi_0 \cdot (0.095 - 0.905) + (\pi_1 + \pi_2) \cdot (0.745 - 0.255) \\
    &= 0.383611759 \cdot (-0.81) + (0.1542805726 + 0.4621076684) \cdot 0.49 \\
    &= 0.383611759 \cdot (-0.81) + 0.616388241 \cdot 0.49 \\
    % &= 0.383611759 \cdot (0.095 - 0.905) + (0.1542805726 + 0.4621076684) \cdot (0.745 - 0.255) \\
    &= -0.0086952867
\end{align*}



\end{document}
